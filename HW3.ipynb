{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPsgwxLU2Sed"
   },
   "source": [
    "# Homework 3\n",
    "\n",
    "DUE DEC 1st at 11:59 PM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgdeprwQ2See"
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "In this problem, you will implement a simple feed-forward neural network using PyTorch, a straight-forward and simple-to-pickup framework for quickly prototyping deep learning model. \n",
    "\n",
    "PyTorch provides 2 powerful things. First, a nice data structure called Tensor (basically a matrix, similar to Numpy ndarray). Tensor is optimized for matrix calculation and can be loaded to a GPU. Tensor is also implemented so that it's easy to calculate and pass back chains of gradients, which is extremely useful for backpropagation on neural network. Second, a nice inner mechanism called Autograd that nicely map variables involved a chain of calculations and efficiently calculate their gradients via the chain rule when needed. Read more here: https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95  \n",
    "\n",
    "You will train and evaluate the model on a simple dataset for classifying seeds. The dataset has 7 features and 3 labels. Refer to the example from the discussion session if needed. \n",
    "\n",
    "**In this problem, you are given a good amount of freedom in tweaking your choices in designing the network. As long as it's still a feed-forward network, it's fine. Accuracy does not matter as long as it's reasonable (like above 80%) but do try to achieve a high accuracy.**\n",
    "\n",
    "(Optional): the dataset provided to you is very simple. Instead, you can challenge yourself by trying more difficult datasets. Some datasets that you can loaded directly using PyTorch:\n",
    "- MNIST (Most famous dataset for getting into Deep Learning)\n",
    "- Fashion-MNIST\n",
    "- Kuzushiji-MNIST \n",
    "\n",
    "The datasets are available at https://pytorch.org/docs/stable/torchvision/datasets.html. You will get the same grade no matter what dataset you use. \n",
    "\n",
    "### Part a\n",
    "Firstly, load and inspect the dataset from \"seeds_dataset.csv\". Split them into a train set (90%) and a test set (10%). You should be quite comfortable with these operations by now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3eztCAG02See"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "sys.path.append(\"/home/nbuser/.local/lib/python2.7/site-packages\")\n",
    "\n",
    "seeds_ds = pd.read_csv('/Users/allisonyih/Desktop/seeds_dataset.csv')\n",
    "X = seeds_ds.iloc[:,:-1].to_numpy(dtype=np.float32)\n",
    "y = seeds_ds.iloc[:,-1].astype(\"category\").cat.codes.to_numpy(dtype=np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KpCu45p2See"
   },
   "source": [
    "### Part b \n",
    "Create a Python class for our neural network model. The network should have 1 input layer, at least 1 hidden layer, and 1 output layer. You are free to choose the size and the number of hidden layers (it may affect the performance so try tweaking around a bit), and the activation function (or no activation at all).\n",
    "\n",
    "Some popular activation functions that you can try:\n",
    "- Sigmoid (torch.sigmoid)\n",
    "- ReLU (torch.relu)\n",
    "- Tanh (torch.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uH6tsTUr2See"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(network,self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=7, out_features=100)\n",
    "        self.layer2 = torch.nn.Linear(in_features=100, out_features=100)\n",
    "        self.outputlayer = torch.nn.Linear(in_features=100, out_features=10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.outputlayer(x)\n",
    "        return x\n",
    "    \n",
    "    # Create the network class by filling in this block of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebDMfYee2See"
   },
   "source": [
    "### Part c \n",
    "Train the network using the train dataset. You are free to choose any suitable optimizer and loss function provided by PyTorch (or you can just use SGD optimizer and CrossEntropyLoss like we did in the discussion session for simplicity). After each epoch, record the current loss and the current accuracy. The current accuracy is obtained by evaluating the model on the train dataset. \n",
    "\n",
    "Some optimizers that you can try:\n",
    "- SGD\n",
    "- Adagrad\n",
    "- Adam\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VTCFkzmm2See"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Current loss:  1.1903793811798096\n",
      "Epoch: 10 Current loss:  0.8252061605453491\n",
      "Epoch: 20 Current loss:  0.611474335193634\n",
      "Epoch: 30 Current loss:  0.5715062022209167\n",
      "Epoch: 40 Current loss:  0.45061159133911133\n",
      "Epoch: 50 Current loss:  0.4355330765247345\n",
      "Epoch: 60 Current loss:  0.5102429389953613\n",
      "Epoch: 70 Current loss:  0.44613102078437805\n",
      "Epoch: 80 Current loss:  0.41728654503822327\n",
      "Epoch: 90 Current loss:  0.3320446312427521\n",
      "Epoch: 100 Current loss:  0.31742095947265625\n",
      "Epoch: 110 Current loss:  0.19667980074882507\n",
      "Epoch: 120 Current loss:  0.2316598743200302\n",
      "Epoch: 130 Current loss:  0.33980438113212585\n",
      "Epoch: 140 Current loss:  0.2882339060306549\n",
      "Epoch: 150 Current loss:  0.18681159615516663\n",
      "Epoch: 160 Current loss:  0.27938342094421387\n",
      "Epoch: 170 Current loss:  0.2940530776977539\n",
      "Epoch: 180 Current loss:  0.27780887484550476\n",
      "Epoch: 190 Current loss:  0.2864077389240265\n"
     ]
    }
   ],
   "source": [
    "#LEARNING_RATE = 0.01\n",
    "#EPOCHS = 20\n",
    "\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "\n",
    "# Create the neural network\n",
    "network = network()\n",
    "# Define the loss function and the optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate) # Named SGD but actually mini-batch Gradient Descent\n",
    "\n",
    "# Set network in training mode\n",
    "network.train()\n",
    "\n",
    "training_size = len(X_train)\n",
    "num_batches = math.ceil(training_size/batch_size)\n",
    "epoch_list = []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        \n",
    "        X_train_batch = torch.tensor(X_train[batch*batch_size:min(batch*batch_size+batch_size,training_size),:])\n",
    "        y_train_batch = torch.tensor(y_train[batch*batch_size:min(batch*batch_size+batch_size,training_size)], dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = network(X_train_batch)\n",
    "        \n",
    "        loss = criterion(output,y_train_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch % 10) == 0:\n",
    "        print(\"Epoch:\", epoch, \"Current loss: \", loss.item())\n",
    "        epoch_list.append(epoch)\n",
    "        loss_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoVUZHu02See"
   },
   "source": [
    "Plot how the loss and the accuracy change over the epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R4T-5QDs2See"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a24b01c10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASyklEQVR4nO3dfYxcV3nH8e/DxqHbEnDBpkrWDjaVsYqA1tEqiuRCw0uwE7WxSyuaFAqUgEVF2iJaC1upUhT+IGC1VZFSqEujAIIEUI1rlSBTNbRUgFNvcBLnhQ0mBLLrNFkChkrZEsd9+sfcDePN7O6sZ2bvzNnvR1p55tzrmUdnd39759xzz43MRJI0+J5VdwGSpO4w0CWpEAa6JBXCQJekQhjoklSIs+p641WrVuW6devqentJGkh33HHHDzJzdatttQX6unXrGBsbq+vtJWkgRcT35trmkIskFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqxIKBHhE3RsRjEXHPHNvfFBF3V19fj4hf7X6ZkqSFtHOEfhOwdZ7t3wV+IzNfAXwA2NuFuiRJi7TghUWZ+dWIWDfP9q83PT0ErOm8rNb2H5lkz8Fxjp+Y5ryVw+zcspHtm0Z69XaSNFC6faXoVcCX5toYETuAHQDnn3/+ol54/5FJdu87yvTJUwBMnphm976jAIa6JNHFk6IR8Woagf6+ufbJzL2ZOZqZo6tXt1yKYE57Do4/HeYzpk+eYs/B8TMpV5KK05Uj9Ih4BfBx4NLMfLwbrznb8RPTi2qXpOWm4yP0iDgf2Af8QWY+0HlJrZ23cnhR7ZK03LQzbfFm4BvAxoiYiIirIuJdEfGuapdrgRcAfxcRd0ZET5ZQ3LllI8Mrhk5rG14xxM4tG3vxdpI0cNqZ5XLlAtvfAbyjaxXNYebEp7NcJKm12tZDPxPbN40Y4JI0By/9l6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEWDPSIuDEiHouIe+bYHhHxkYg4FhF3R8QF3S9TkrSQdo7QbwK2zrP9UmBD9bUD+GjnZUmSFmvBQM/MrwI/nGeXbcAns+EQsDIizu1WgZKk9nRjDH0EeLjp+UTV9gwRsSMixiJibGpqqgtvLUma0Y1AjxZt2WrHzNybmaOZObp69eouvLUkaUY3An0CWNv0fA1wvAuvK0lahG4E+gHgLdVsl4uAH2fmI114XUnSIpy10A4RcTNwMbAqIiaAvwRWAGTmx4BbgcuAY8ATwB/2qlhJ0twWDPTMvHKB7Qm8u2sVSZLOiFeKSlIhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1Ihzqq7gKW0/8gkew6Oc/zENOetHGbnlo1s3zRSd1mS1BXLJtD3H5lk976jTJ88BcDkiWl27zsKYKhLKsKyGXLZc3D86TCfMX3yFHsOjtdUkSR117IJ9OMnphfVLkmDZtkE+nkrhxfVLkmDpq1Aj4itETEeEcciYleL7edHxFci4khE3B0Rl3W/1M7s3LKR4RVDp7UNrxhi55aNNVUkSd214EnRiBgCbgAuASaAwxFxIDPva9rtL4DPZeZHI+KlwK3Auh7Ue8ZmTnw6y0VSqdqZ5XIhcCwzHwSIiFuAbUBzoCfw3Orx84Dj3SyyW7ZvGjHAJRWrnSGXEeDhpucTVVuz9wNvjogJGkfnf9zqhSJiR0SMRcTY1NTUGZQrSZpLO4EeLdpy1vMrgZsycw1wGfCpiHjGa2fm3swczczR1atXL75aSdKc2gn0CWBt0/M1PHNI5SrgcwCZ+Q3g54BV3ShQktSedgL9MLAhItZHxNnAFcCBWft8H3gtQET8Co1Ad0xFkpbQgoGemU8BVwMHgftpzGa5NyKui4jLq93+DHhnRNwF3Ay8LTNnD8tIknqorbVcMvNWGic7m9uubXp8H7C5u6VJkhZj2VwpKkmlM9AlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQV4qy6Cxgk+49MsufgOMdPTHPeymF2btnI9k0jdZclSYCB3rb9RybZve8o0ydPATB5Yprd+44CGOqS+kJbQy4RsTUixiPiWETsmmOfN0bEfRFxb0R8prtl1m/PwfGnw3zG9MlT7Dk4XlNFknS6BY/QI2IIuAG4BJgADkfEgcy8r2mfDcBuYHNm/igiXtirguty/MT0otolaam1c4R+IXAsMx/MzCeBW4Bts/Z5J3BDZv4IIDMf626Z9Ttv5fCi2iVpqbUT6CPAw03PJ6q2Zi8BXhIRX4uIQxGxtdULRcSOiBiLiLGpqakzq7gmO7dsZHjF0GltwyuG2LllY00VSdLp2jkpGi3assXrbAAuBtYA/xkRL8vME6f9p8y9wF6A0dHR2a/R12ZOfDrLRVK/aifQJ4C1Tc/XAMdb7HMoM08C342IcRoBf7grVfaJ7ZtGDHBJfaudQD8MbIiI9cAkcAXw+7P22Q9cCdwUEatoDME82M1CS+A8dkm9tGCgZ+ZTEXE1cBAYAm7MzHsj4jpgLDMPVNteHxH3AaeAnZn5eC8LHzTOY5fUa5FZz1D26Ohojo2N1fLeddh8/W1MtpjiOLJymK/tek0NFUkaRBFxR2aOttrmWi5LxHnsknrNQF8izmOX1GsG+hJxHrukXnNxriXiPHZJvWagLyHnsUvqJYdcJKkQBrokFcJAl6RCOIauRXH5Aql/Gehqm8sXSP3NIRe1zdvwSf3NQFfbXL5A6m8Gutrm8gVSfzPQ1TaXL5D6mydF1TaXL5D6m4GuRXH5Aql/OeQiSYUw0CWpEAa6JBXCQJekQnhSdMC4loqkuRjoA6SEtVT8gyT1jkMuA2TQ11KZ+YM0eWKa5Gd/kPYfmay7NKkIHqEPkG6spVLnEfJ8f5A8Spc65xH6AOl0LZW6j5Bd3EvqLQN9gHS6lkrdQzYu7iX1loE+QLZvGuGDb3g5IyuHCWBk5TAffMPL2x6uqPsIuRuLe+0/Msnm629j/a4vsvn62xx/l5o4hj5gOllL5byVw0y2CO+lOkLudHGvEmb5SL1koC8jO7dsPC0QYemXv+3kD5InVaX5tTXkEhFbI2I8Io5FxK559vvdiMiIGO1eieqWTods6lb3kJHU7xY8Qo+IIeAG4BJgAjgcEQcy875Z+50D/Alwey8KVXcM8vK3dQ8ZSf2unSP0C4FjmflgZj4J3AJsa7HfB4APA//bxfqkp3nHJGl+7QT6CPBw0/OJqu1pEbEJWJuZ/zLfC0XEjogYi4ixqampRRer5W3Qh4ykXmvnpGi0aMunN0Y8C/gb4G0LvVBm7gX2AoyOjuYCu0vPMMhDRlKvtXOEPgGsbXq+Bjje9Pwc4GXAv0fEQ8BFwAFPjErS0mon0A8DGyJifUScDVwBHJjZmJk/zsxVmbkuM9cBh4DLM3OsJxVLklpaMNAz8yngauAgcD/wucy8NyKui4jLe12gJKk9bV1YlJm3ArfOart2jn0v7rwsSdJiuZaLJBXCQJekQhjoklQIF+eSlhHv6Vo2A13LynIONJcfLp9DLlo26r4FX93qvmOVes9A17Kx3APN5YfL55CLlo1uBNogD9m4/HD5PELXstHpTaoHfcjG5YfLZ6Br2eg00AZ9yMblh8vnkIuWjU5vUl3CGLTLD5fNQNey0kmgOQatfueQi9Qmx6AH3/4jk2y+/jbW7/oim6+/bWDOf7TLI3SpTZ0O2ZRgkGf59MOFVb3uv8is505wo6OjOTbmPTCkQTE7EKHxCWVQTqxuvv62lkNmIyuH+dqu1/T8/bvVfxFxR2a2vCOcR+iS2jLfLJ9BOMKt+zqEpeg/A11SW+qe5dPpkEmnJ7U7ff+l6D9PikpqS6cXZnWq0+sA6r4OYSn6z0CX1Ja6Z/l0eoTb6YVVnb7/UvSfQy6S2lL3LJ9uXAdQ53UIS9F/BrqkttV5penOLRtbzhJZqk8I3Xj/XvefgS5pINT9CaHu92+H89AlaYDMNw/dk6KSVAiHXKQBMsiX3qv3DHRpQPTDWiTqbw65SANi0G+wod4z0KUBUfel9+p/Bro0IOq+9F79z0CXBkTdl96r/7V1UjQitgJ/CwwBH8/M62dtfy/wDuApYAp4e2Z+r8u1SgOvk1kqg3Bhi+q14IVFETEEPABcAkwAh4ErM/O+pn1eDdyemU9ExB8BF2fm7833ul5YpOVm0G8Qof7Q6YVFFwLHMvPBzHwSuAXY1rxDZn4lM5+onh4C1nRSsFQiZ6mUf0/PurUT6CPAw03PJ6q2uVwFfKnVhojYERFjETE2NTXVfpVSAZb7LJWZTyiTJ6ZJfjaP3lDvnnYCPVq0tRyniYg3A6PAnlbbM3NvZo5m5ujq1avbr1IqwHKfpeInlN5rJ9AngLVNz9cAx2fvFBGvA64BLs/Mn3anPKkcy32WynL/hLIU2gn0w8CGiFgfEWcDVwAHmneIiE3A39MI88e6X6Y0+Dq9Y86gW+6fUJbCgtMWM/OpiLgaOEhj2uKNmXlvRFwHjGXmARpDLM8BPh8RAN/PzMt7WLc0kOq8QUTd6r5BxXLQ1jz0zLwVuHVW27VNj1/X5bokFcZ59L3naouSlsxy/oSyFLz0X5IKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklSIBW9w0bM3jpgCzvSuRquAH3SxnG7r9/qg/2u0vs5YX2f6ub4XZWbL5WprC/RORMTYXHfs6Af9Xh/0f43W1xnr60y/1zcXh1wkqRAGuiQVYlADfW/dBSyg3+uD/q/R+jpjfZ3p9/paGsgxdEnSMw3qEbokaRYDXZIKMXCBHhFbI2I8Io5FxK4+qGdtRHwlIu6PiHsj4k+r9vdHxGRE3Fl9XVZjjQ9FxNGqjrGq7fkR8a8R8e3q31+sqbaNTX10Z0T8JCLeU2f/RcSNEfFYRNzT1Nayv6LhI9XP490RcUFN9e2JiG9VNXwhIlZW7esiYrqpHz9WU31zfj8jYnfVf+MRsaWm+j7bVNtDEXFn1b7k/deRzByYLxr3NP0O8GLgbOAu4KU113QucEH1+BzgAeClwPuBP6+7z6q6HgJWzWr7MLCrerwL+FAf1DkE/Dfwojr7D3gVcAFwz0L9BVwGfAkI4CLg9prqez1wVvX4Q031rWver8b+a/n9rH5X7gKeDayvfr+Hlrq+Wdv/Cri2rv7r5GvQjtAvBI5l5oOZ+SRwC7CtzoIy85HM/Gb1+H+A+4FBuMfWNuAT1eNPANtrrGXGa4HvZOaZXkHcFZn5VeCHs5rn6q9twCez4RCwMiLOXer6MvPLmflU9fQQsKaXNcxnjv6byzbglsz8aWZ+FzhG4/e8Z+arLxp3uX8jcHMva+iVQQv0EeDhpucT9FF4RsQ6YBNwe9V0dfUR+Ma6hjQqCXw5Iu6IiB1V2y9l5iPQ+KMEvLC26n7mCk7/ReqX/oO5+6sffybfTuNTw4z1EXEkIv4jIl5ZV1G0/n72W/+9Eng0M7/d1NYv/begQQv0aNHWF/MuI+I5wD8B78nMnwAfBX4Z+DXgERof4+qyOTMvAC4F3h0Rr6qxlpYi4mzgcuDzVVM/9d98+upnMiKuAZ4CPl01PQKcn5mbgPcCn4mI59ZQ2lzfz77qP+BKTj+o6Jf+a8ugBfoEsLbp+RrgeE21PC0iVtAI809n5j6AzHw0M09l5v8B/0CPP0bOJzOPV/8+BnyhquXRmaGB6t/H6qqvcinwzcx8FPqr/ypz9Vff/ExGxFuB3wTelNUAcDWU8Xj1+A4aY9QvWera5vl+9lP/nQW8AfjsTFu/9F+7Bi3QDwMbImJ9dUR3BXCgzoKqMbd/BO7PzL9uam8eR/1t4J7Z/3cpRMQvRMQ5M49pnDy7h0a/vbXa7a3AP9dRX5PTjoz6pf+azNVfB4C3VLNdLgJ+PDM0s5QiYivwPuDyzHyiqX11RAxVj18MbAAerKG+ub6fB4ArIuLZEbG+qu+/lrq+yuuAb2XmxExDv/Rf2+o+K7vYLxqzCh6g8Zfymj6o59dpfES8G7iz+roM+BRwtGo/AJxbU30vpjGL4C7g3pk+A14A/Bvw7erf59fYhz8PPA48r6mttv6j8YflEeAkjSPIq+bqLxpDBjdUP49HgdGa6jtGYyx65mfwY9W+v1N93+8Cvgn8Vk31zfn9BK6p+m8cuLSO+qr2m4B3zdp3yfuvky8v/ZekQgzakIskaQ4GuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSrE/wMxLN8S/fyg2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(epoch_list, loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvFzNDRT2See"
   },
   "source": [
    "### Part d \n",
    "Evaluate the model on the test dataset. Print out the accuracy. Does this accuracy agrees with the training accuracy showed on the plot? Why may they be different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t0eC-Wi32See"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "network.eval()\n",
    "\n",
    "pred = torch.argmax(network(torch.tensor(X_test)), dim=1).numpy()\n",
    "\n",
    "print(\"Accuracy: \", np.sum(pred == y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy6C2pFfyXIa"
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "If you haven't done so already, please read Chapter 2 (on fairness) of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe6qfhRFyXIa"
   },
   "source": [
    "### Part a\n",
    "\n",
    "Import the 'semi_synthetic.csv' dataset. This dataset is a modified version of the UCI Credit card dataset (details [here](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)). We added a new synthetic feature to this dataset called `LIMIT_BAL`. It is highly predictive for `SEX==2` but not at all for `SEX==1`. As it turns out, this causes issues with fairness where models rely too much on `LIMIT_BAL` and discriminate against `SEX==2`. We will try to mitigate this in terms of the fairness criterion 'Statistical Parity' as it's mentioned in the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HaqdFHPvyXIa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_1</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.897646</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.239472</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.259719</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.277865</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.257674</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0.714882</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>88004</td>\n",
       "      <td>31237</td>\n",
       "      <td>15980</td>\n",
       "      <td>8500</td>\n",
       "      <td>20000</td>\n",
       "      <td>5003</td>\n",
       "      <td>3047</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0.314780</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8979</td>\n",
       "      <td>5190</td>\n",
       "      <td>0</td>\n",
       "      <td>1837</td>\n",
       "      <td>3526</td>\n",
       "      <td>8998</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>-0.930758</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20878</td>\n",
       "      <td>20582</td>\n",
       "      <td>19357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22000</td>\n",
       "      <td>4200</td>\n",
       "      <td>2000</td>\n",
       "      <td>3100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>-0.087740</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52774</td>\n",
       "      <td>11855</td>\n",
       "      <td>48944</td>\n",
       "      <td>85900</td>\n",
       "      <td>3409</td>\n",
       "      <td>1178</td>\n",
       "      <td>1926</td>\n",
       "      <td>52964</td>\n",
       "      <td>1804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0.126607</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36535</td>\n",
       "      <td>32428</td>\n",
       "      <td>15313</td>\n",
       "      <td>2078</td>\n",
       "      <td>1800</td>\n",
       "      <td>1430</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_1  PAY_2  PAY_3  PAY_4  \\\n",
       "0       0.897646    2          2         1   24      2      2     -1     -1   \n",
       "1       1.239472    2          2         2   26     -1      2      0      0   \n",
       "2      -0.259719    2          2         2   34      0      0      0      0   \n",
       "3      -0.277865    2          2         1   37      0      0      0      0   \n",
       "4      -0.257674    1          2         1   57     -1      0     -1      0   \n",
       "...          ...  ...        ...       ...  ...    ...    ...    ...    ...   \n",
       "29995   0.714882    1          3         1   39      0      0      0      0   \n",
       "29996   0.314780    1          3         2   43     -1     -1     -1     -1   \n",
       "29997  -0.930758    1          2         2   37      4      3      2     -1   \n",
       "29998  -0.087740    1          3         1   41      1     -1      0      0   \n",
       "29999   0.126607    1          2         1   46      0      0      0      0   \n",
       "\n",
       "       PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0         -2  ...          0          0          0         0       689   \n",
       "1          0  ...       3272       3455       3261         0      1000   \n",
       "2          0  ...      14331      14948      15549      1518      1500   \n",
       "3          0  ...      28314      28959      29547      2000      2019   \n",
       "4          0  ...      20940      19146      19131      2000     36681   \n",
       "...      ...  ...        ...        ...        ...       ...       ...   \n",
       "29995      0  ...      88004      31237      15980      8500     20000   \n",
       "29996      0  ...       8979       5190          0      1837      3526   \n",
       "29997      0  ...      20878      20582      19357         0         0   \n",
       "29998      0  ...      52774      11855      48944     85900      3409   \n",
       "29999      0  ...      36535      32428      15313      2078      1800   \n",
       "\n",
       "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0             0         0         0         0                           1  \n",
       "1          1000      1000         0      2000                           1  \n",
       "2          1000      1000      1000      5000                           0  \n",
       "3          1200      1100      1069      1000                           0  \n",
       "4         10000      9000       689       679                           0  \n",
       "...         ...       ...       ...       ...                         ...  \n",
       "29995      5003      3047      5000      1000                           0  \n",
       "29996      8998       129         0         0                           0  \n",
       "29997     22000      4200      2000      3100                           1  \n",
       "29998      1178      1926     52964      1804                           1  \n",
       "29999      1430      1000      1000      1000                           1  \n",
       "\n",
       "[30000 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "semi_synth_ds = pd.read_csv('/Users/allisonyih/Desktop/semi_synthetic.csv')\n",
    "semi_synth_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJhtmsUHyXIa"
   },
   "source": [
    "Extract features from the dataset (all columns except last) and call it `X`. Extract last column and call it `Y`. Also extract the `SEX` column and call this `SF` (for sensitive feature). Split all of these arrays data into train and test sets. You can use `sklearn.model_selection.train_test_split` and 20% ratio for the test dataset. Pass `stratify=Y` to `train_test_split` if you're using it. This ensures that both test and train datasets have the same ratio of 0/1 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4RWhQl8zyXIa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(2020)\n",
    "np.random.seed(2020)\n",
    "\n",
    "S = semi_synth_ds[['SEX']].astype(int)\n",
    "# Change value to represent age groups\n",
    "S['SEX'] = np.where(S.SEX > 1, 'FEMALE', 'MALE')\n",
    "\n",
    "X = semi_synth_ds.iloc[:,:-1].to_numpy(dtype=np.float32)\n",
    "Y = semi_synth_ds.iloc[:,-1].astype(\"category\").cat.codes.to_numpy(dtype=np.float32)\n",
    "X_train1, X_test1, y_train1, y_test1, S_train, S_test, = train_test_split(X, Y, S, stratify=Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_kwHg4SyXIa"
   },
   "source": [
    "Use a random forest classifier (whichever hyperparameters you want, you may leave it default) and train it using the train dataset (only `X_train` and `Y_train`. We will not use `SF_train` yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YESO3KAfyXIa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy from training: 0.8614166666666667\n",
      "Best parameters from training: {'max_depth': 11, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, ParameterGrid, cross_val_score, cross_validate\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [5, 10, 15, 20],'max_depth': [2, 5, 8, 11]}\n",
    "\n",
    "rf_clf = GridSearchCV(estimator = rf, param_grid = parameters, cv = 10)\n",
    "\n",
    "# Train the classifier on training's feature and target data\n",
    "rf_clf.fit(X_train1, y_train1)\n",
    "print('Best accuracy from training:', rf_clf.best_score_) \n",
    "print('Best parameters from training:',rf_clf.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U4MeVC3yXIa"
   },
   "source": [
    "### Part b\n",
    "\n",
    "Now we will consider the 'statistical parity' fairness metric on the test dataset. This basically means we compare the selection rates (i.e. predicted labels being 1) conditioned on sex and look at their difference/ratio. To do this, print 5 things: \n",
    "\n",
    "1. Overall selection rate (what percent is predicted as 1)\n",
    "2. Selection rate for `SEX==1`\n",
    "3. Selection rate for `SEX==2`\n",
    "4. Difference between 2. and 3.\n",
    "5. Ratio of 2. and 3.\n",
    "\n",
    "Note that we didn't use anything related to how accurate the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qQcj-_PyXIa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1Yn6sXXyXIa"
   },
   "source": [
    "There is a new Python package called `fairlearn` that aims to help machine learning researchers use fairness techniques more easily. Go ahead and download this package (`pip install fairlearn` or `conda install fairlearn`). Note: the authors of `fairlearn` call 'statistical parity' from the textbook as 'demographic parity'.\n",
    "\n",
    "We will use `selection_rate`, `demographic_parity_difference`, `demographic_parity_ratio` from `fairlearn.metrics` to calculate 1., 4. and 5. above. The way to use these functions is: first argument `Y_test`, second argument `Y_prediction`, and for `demographic_parity_difference` and `demographic_parity_ratio` you need to give a keyword argument for the 'sensitive feature' by `sensitive_features=SF_test`.\n",
    "\n",
    "Use these three functions and confirm your earlier calculations for 1., 4. and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I27r9Mx1yXIa"
   },
   "outputs": [],
   "source": [
    "from fairlearn.metrics import selection_rate, demographic_parity_difference, demographic_parity_ratio, MetricFrame\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "y_hat = rf_clf.fit(X_train1, y_train1).predict(X_test1)\n",
    "overall_selection_rate = selection_rate(y_test1, y_hat)\n",
    "print(overall_selection_rate)\n",
    "\n",
    "metrics = {'selection_rate': selection_rate, 'accuracy': accuracy_score}\n",
    "group_metrics = MetricFrame(metrics,y_test1, y_hat,S_test['SEX'])\n",
    "print(group_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XViy0I5EyXIa"
   },
   "source": [
    "### Part c\n",
    "### Mitigating Unfairness with Giving Weights to Datapoints\n",
    "\n",
    "Now we are going to use the `GridSearch` function from `fairlearn.reductions` to try to mitigate this statistical parity discrepancy. This is one of the many ways to achieve this. What this function does is it gives higher weights to certain datapoints (determined by the sensitive feature) in order to change what the model learns. It automatically uses grid search to determine which weight should be given to `SEX==1` datapoints vs `SEX==2` datapoints (if you're interested you can read the source code). While it is called grid search, the space it searches is 1-dimensional. For our purposes, `GridSearch` takes four arguments: \n",
    "\n",
    "1. The model you defined above. This is a non-keyword argument.\n",
    "2. Constraint. This needs to be an object defined by `fairlearn`, the class we will be using is `fairlearn.reductions.DemographicParity`. It corresponds to minimizing the difference in selection rates (4. from above). This is a non-keyword argument.\n",
    "3. (Keyword argument) `grid_limit`. This determines the boundaries of the grid search. Basically, the `GridSearch` function will search the interval \\[-`grid_limit`,`grid_limit`\\] to find the weights. Choose this to be 0.8.\n",
    "3. (Keyword argument) `grid_size`. This determines how many points there will be in the grid search. Choose this to be 21.\n",
    "\n",
    "Just like `sklearn.model_selection.GridSearchCV` we have seen in HW1, `fairlearn.reductions.GridSearch` is also a wrapper, so you need to call `.fit()` on this wrapper (not the model directly). Training should be done in less than 5 minutes. In the end this wrapper will contain `grid_size` many models, each corresponding to a different weight for datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67Y6ezsJyXIa"
   },
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch, DemographicParity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15EO5RaTyXIa"
   },
   "source": [
    "The `GridSearch` wrapper object you trained has a field called `predictors_`. This gives a list of all the models trained in this grid search, which will be of size `grid_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7HdpQm_yXIa"
   },
   "outputs": [],
   "source": [
    "predictions_sweep = [predictor.predict(X_test) for predictor in gridsearch_wrapper.predictors_] \n",
    "accuracy_sweep = [predictor.score(X_test, Y_test) for predictor in gridsearch_wrapper.predictors_] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXGkicwxyXIa"
   },
   "source": [
    "Notice that `predictions_sweep` is a 21 element list. Each of these elements is a numpy array that contains the predictions (`Y_pred`) for `X_test` for that particular model. Use `demographic_parity_difference` like you did above to find the difference in selection rates for each of these models and store these 'fairness metrics' in a numpy array (it too will be size 21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZ-bxFwmyXIa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydkOWKGJyXIa"
   },
   "source": [
    "Plot the accuracy vs difference in selection rates for all these models in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzW_0fq2yXIa"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLrO9mVFyXIa"
   },
   "source": [
    "### Part d \n",
    "\n",
    "Now calculate the *pareto front*: go over each (accuracy, difference in selection rates) pair and compare it to every other pair. If you find the first pair to have lower accuracy and higher difference in selection rate, discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ElVQmrHyXIa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23SaZvzSyXIa"
   },
   "source": [
    "* Plot the accuracy vs difference in selection rates for all the models the grid search in a scatter plot.\n",
    "* On the same plot, plot the accuracy vs difference in selection rates for the points on the pareto front. Make these points red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGAsufEOyXIa"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKA_zKOByXIa"
   },
   "source": [
    "Which one of these models would you choose to use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-qHCrNsyXIa"
   },
   "source": [
    "## Problem 3\n",
    "\n",
    "This problem builds on ideas from paper 2a in Module 1: “Against prediction: Sentencing, Policing, And Punishing In An Actuarial Age,” Chicago Public Law And Legal Theory Working Paper No. 94, Harcourt, 2005. \n",
    "\n",
    "Suppose we have two groups $A$ and $B$ of population sizes $n$ and $9n$. We find that under equal surveillance, group $A$’s offending rate is $2$ offenses per month per person and that of group $B$ is $1$ offense per person per month (See Figure on page 17 of the above paper). Equal surveillance means that out of a total of $10m$ monitors, $m$ monitors are assigned to group $A$ and $9m$ monitors are assigned to group $B$. The surveillance is $\\frac{m}{n}$ per capita for both groups. So, under equal surveillance, the combined offending rate over both groups is $11n$ per month. \n",
    "\n",
    "Suppose we want to bring down the overall offending rate by increasing the monitors for group $A$. In order to do that we collected some data and found that we could model the rate of decrease in offenses for group $A$ by a function as follows:\n",
    "\tIf we increase the per capita surveillance from $\\frac{m}{n}$ to $\\frac{m}{n} (1 + \\delta_A)$, the offending rate declines to $2e^{- \\alpha \\delta_A}$. ($\\alpha>0$)\n",
    "\n",
    "Similarly, we modeled the rate of increase in offenses for group $B$ by a function under reduced surveillance as follows:\n",
    "\tIf we decrease the per capita monitoring rate from $\\frac{m}{n}$ to $\\frac{m}{n} (1 - \\delta_B)$, the offending rate rises to $e^{ \\beta \\delta_B}$. ($\\beta>0$)\n",
    "\n",
    "How would you distribute the monitors so that \n",
    "\n",
    "**a)** Both groups had the same offending rate?\n",
    "\n",
    "**b)** The combined offending rate was minimized?\n",
    "\n",
    "\n",
    "Consider 3 different scenarios when $\\alpha > \\beta$, $\\alpha = \\beta$, and $\\alpha < \\beta$. \n",
    "\n",
    "**c)** For each of the scenarios, which of the above solutions would you prefer and why?\n",
    "\n",
    "**TIPS**:\n",
    "- For **part a** and **part b**, answer by calculating $\\delta_A$ and $\\delta_B$.\n",
    "- Try to set up equations from the given information. This problem mostly consists of algebra.\n",
    "- If you do it correctly, **part c** should makes sense immediately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY2pVtwPyXIa"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
